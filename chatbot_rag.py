# chatbot_rag_faiss.py

import os
import streamlit as st
import fitz  # PyMuPDF ƒë·ªÉ ƒë·ªçc file PDF
import google.generativeai as genai
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain.chains.question_answering import load_qa_chain
from dotenv import load_dotenv

# ----------------- 0. Load API Key t·ª´ .env -----------------
load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

if not GOOGLE_API_KEY:
    st.error("GOOGLE_API_KEY kh√¥ng t√¨m th·∫•y trong .env. Vui l√≤ng c·∫•u h√¨nh tr∆∞·ªõc.")
    st.stop()

genai.configure(api_key=GOOGLE_API_KEY)

# ----------------- 1. UI: Cho ph√©p ng∆∞·ªùi d√πng upload PDF -----------------
st.set_page_config(page_title="Chatbot Ch√≠nh s√°ch C√¥ng ty", page_icon="ü§ñ")
st.header("ü§ñ Chatbot Ch√≠nh s√°ch C√¥ng ty (Powered by Gemini)")
st.subheader("T·∫£i file PDF v·ªÅ ch√≠nh s√°ch v√† h·ªèi chatbot v·ªÅ n·ªôi dung b√™n trong.")

uploaded_files = st.file_uploader("üìÑ T·∫£i l√™n c√°c file PDF", type=["pdf"], accept_multiple_files=True)

# ----------------- 2. Tr√≠ch xu·∫•t n·ªôi dung t·ª´ PDF -----------------
def extract_text_from_pdfs(files):
    documents = []
    for file in files:
        pdf_doc = fitz.open(stream=file.read(), filetype="pdf")
        text = ""
        for page in pdf_doc:
            text += page.get_text()
        documents.append(text)
    return documents

# ----------------- 3. T·∫°o Vector Store t·ª´ PDF -----------------
@st.cache_resource
def get_vector_store_from_pdfs(files):
    if not files:
        return None
    texts = extract_text_from_pdfs(files)
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    vector_store = FAISS.from_texts(texts, embeddings)
    return vector_store

# N·∫øu ng∆∞·ªùi d√πng kh√¥ng upload, s·ª≠ d·ª•ng d·ªØ li·ªáu m·∫∑c ƒë·ªãnh (v√≠ d·ª• m·∫´u)
default_docs = [
    "Ch√≠nh s√°ch ngh·ªâ ph√©p: M·ªói nh√¢n vi√™n ƒë∆∞·ª£c h∆∞·ªüng 15 ng√†y ngh·ªâ ph√©p c√≥ l∆∞∆°ng m·ªói nƒÉm. C√°c ng√†y ngh·ªâ ph√©p kh√¥ng s·ª≠ d·ª•ng s·∫Ω ƒë∆∞·ª£c chuy·ªÉn sang nƒÉm sau t·ªëi ƒëa 5 ng√†y.",
    "Quy tr√¨nh xin ngh·ªâ ph√©p: Nh√¢n vi√™n ph·∫£i n·ªôp ƒë∆°n xin ngh·ªâ ph√©p qua h·ªá th·ªëng n·ªôi b·ªô √≠t nh·∫•t 3 ng√†y l√†m vi·ªác tr∆∞·ªõc ng√†y ngh·ªâ d·ª± ki·∫øn. Ph√™ duy·ªát cu·ªëi c√πng thu·ªôc v·ªÅ qu·∫£n l√Ω tr·ª±c ti·∫øp.",
    "Ch√≠nh s√°ch l√†m th√™m gi·ªù: M·ªçi c√¥ng vi·ªác l√†m th√™m gi·ªù ph·∫£i ƒë∆∞·ª£c qu·∫£n l√Ω tr·ª±c ti·∫øp ph√™ duy·ªát tr∆∞·ªõc. Ti·ªÅn l√†m th√™m gi·ªù ƒë∆∞·ª£c t√≠nh theo h·ªá s·ªë 1.5 l·∫ßn l∆∞∆°ng c∆° b·∫£n cho c√°c ng√†y trong tu·∫ßn v√† 2.0 l·∫ßn cho cu·ªëi tu·∫ßn/ng√†y l·ªÖ.",
    "Quy t·∫Øc l√†m vi·ªác t·ª´ xa: Nh√¢n vi√™n c√≥ th·ªÉ l√†m vi·ªác t·ª´ xa t·ªëi ƒëa 2 ng√†y m·ªói tu·∫ßn, v·ªõi s·ª± ƒë·ªìng √Ω c·ªßa qu·∫£n l√Ω. C·∫ßn ƒë·∫£m b·∫£o k·∫øt n·ªëi internet ·ªïn ƒë·ªãnh v√† m√¥i tr∆∞·ªùng l√†m vi·ªác ph√π h·ª£p.",
    "Ch√≠nh s√°ch b·∫£o m·∫≠t th√¥ng tin: M·ªçi th√¥ng tin kh√°ch h√†ng v√† d·ªØ li·ªáu n·ªôi b·ªô c√¥ng ty l√† t√†i s·∫£n m·∫≠t. Nghi√™m c·∫•m chia s·∫ª ho·∫∑c ti·∫øt l·ªô th√¥ng tin n√†y cho b√™n th·ª© ba d∆∞·ªõi m·ªçi h√¨nh th·ª©c.",
    "Ngh·ªâ kh√¥ng ph√©p: s·∫Ω b·ªã tr·ª´ 1 ng√†y l∆∞∆°ng",
    "ngh·ªâ qu√° 80% ng√†y c√¥ng trong th√°ng s·∫Ω kh√¥ng ƒë∆∞·ª£c t√≠nh l∆∞∆°ng"
]

@st.cache_resource
def get_default_vector_store():
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    return FAISS.from_texts(default_docs, embeddings)

# Ch·ªçn vector store t√πy theo t√¨nh hu·ªëng
vector_store = get_vector_store_from_pdfs(uploaded_files) if uploaded_files else get_default_vector_store()

# ----------------- 4. Kh·ªüi t·∫°o m√¥ h√¨nh Gemini & QA Chain -----------------
llm = ChatGoogleGenerativeAI(model="models/gemini-1.5-flash", temperature=0.2)

prompt_template = """
B·∫°n l√† m·ªôt tr·ª£ l√Ω th√¢n thi·ªán, ch·ªâ tr·∫£ l·ªùi c√°c c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p trong ng·ªØ c·∫£nh sau.
N·∫øu th√¥ng tin kh√¥ng c√≥ trong ng·ªØ c·∫£nh, vui l√≤ng n√≥i r·∫±ng b·∫°n kh√¥ng th·ªÉ t√¨m th·∫•y th√¥ng tin ƒë√≥.
B·∫°n KH√îNG ƒë∆∞·ª£c tr·∫£ l·ªùi b·∫•t k·ª≥ c√¢u h·ªèi n√†o n·∫±m ngo√†i ph·∫°m vi c·ªßa ng·ªØ c·∫£nh n√†y.
Lu√¥n cung c·∫•p c√¢u tr·∫£ l·ªùi r√µ r√†ng v√† ng·∫Øn g·ªçn.

Ng·ªØ c·∫£nh:
{context}

C√¢u h·ªèi:
{question}

Tr·∫£ l·ªùi:
"""

prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
qa_chain = load_qa_chain(llm=llm, chain_type="stuff", prompt=prompt)

# ----------------- 5. H√†m tr·∫£ l·ªùi c√¢u h·ªèi -----------------
def get_gemini_response(question):
    if not vector_store:
        return "Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ t√¨m ki·∫øm. Vui l√≤ng t·∫£i l√™n file PDF."
    docs = vector_store.similarity_search(question, k=2)
    response = qa_chain({"input_documents": docs, "question": question})
    return response["output_text"]

# ----------------- 6. Giao di·ªán Chat -----------------
if "messages" not in st.session_state:
    st.session_state.messages = []

# Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Input ng∆∞·ªùi d√πng
if prompt := st.chat_input("üí¨ B·∫°n mu·ªën h·ªèi g√¨?"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("ü§î ƒêang t√¨m c√¢u tr·∫£ l·ªùi..."):
            response = get_gemini_response(prompt)
            st.markdown(response)

    st.session_state.messages.append({"role": "assistant", "content": response})

# Hi·ªÉn th·ªã file ƒë√£ upload
if uploaded_files:
    st.markdown("---")
    st.success(f"üìö ƒê√£ upload {len(uploaded_files)} file:")
    for f in uploaded_files:
        st.markdown(f"- {f.name}")

st.markdown("---")
st.caption("üí° Chatbot n√†y s·ª≠ d·ª•ng Gemini v√† LangChain ƒë·ªÉ tr·∫£ l·ªùi d·ª±a tr√™n n·ªôi dung file PDF b·∫°n ƒë√£ cung c·∫•p.")
